\documentclass[xcolor=svgnames,dvipdfmx,cjk]{beamer} 
\AtBeginDvi{\special{pdf:tounicode 90ms-RKSJ-UCS2}} 
\usetheme[progressbar=frametitle]{metropolis}
\setbeamertemplate{headline}[miniframes]
\setbeamercolor{background canvas}{bg=Snow} 
\setbeamercolor{frametitle}{fg=white, bg=gray!40!black}  
\setbeamercolor{section in head/foot}{fg=pink!70!red, bg=gray!30!black}
\setbeamercolor{alerted text}{fg=orange!50!red} 
\usefonttheme{professionalfonts}
\setbeamertemplate{theorems}[numbered]
\newtheorem{thm}{Theorem}[section]
\newtheorem{proposition}[thm]{Proposition}
\theoremstyle{example}
\newtheorem{exam}[thm]{Example}
\newtheorem{remark}[thm]{Remark}
\newtheorem{question}[thm]{Question}
\newtheorem{prob}[thm]{Problem}
\def\Avar{\text{Avar}}
\def\var{\text{var}}
\def\Var{\text{Var}}
\def\cov{\text{cov}}
\def\Cov{\text{Cov}}
\def\E{\mathbb{E}}
\def\R{\mathbb{R}}
\def\P{\mathbb{P}}
\def\parrow{\xrightarrow{p}}
\def\darrow{\xrightarrow{d}}
\def\plim{\text{plim }}
\usepackage{bbm}
\usepackage{ascmac}
\usepackage{tcolorbox}

%%%%%%ナビゲーションバー%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\useoutertheme{miniframes}
%\setbeamertemplate{headline}[miniframes theme]
%\setbeamertemplate{mini frames}[circle]

%\setbeamertemplate{headline}
%{%
%  \begin{beamercolorbox}{section in head/foot}
%    \vskip2pt\insertnavigation{\paperwidth}\vskip2pt
%  \end{beamercolorbox}%
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document} 

%%%%%講演に関する情報%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Semiparametric Single Index Models} 
\subtitle{Li and Racine (2007, Chapter 8)}
\author{Yasuyuki Matsumura}
\institute{Graduate School of Economics, Kyoto University}
\date{\today} % 日付を自動で挿入

%%%%%タイトルページ%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}                  
\titlepage            
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Before showwing the contents of the slides...

\begin{frame}{Introduction}
  \begin{itemize}
    \item  \alert{A semiparametric single index model} is given by 
            \begin{align*}
              Y = g (X^{T} \beta_0) + u,
            \end{align*}
           where  
            \begin{align*}
              & Y \in \mathbb{R}: \text{a dependent variable}, \\
              & X \in \mathbb{R}^{q}: \text{a }  q \times 1 \text{ explanatory vector}, \\
              & \beta_0 \in \mathbb{R}^{q}: \text{a }  q \times 1 \text{ vector of unknown parameters}, \\
              & u \in \mathbb{R}: \text{an error term which satisfies } \mathbb{E}(u \mid X) =0, \\
              & g(\cdot): \text{an unknown distribution function}.
            \end{align*}
  \end{itemize}
\end{frame}

\begin{frame}{Introduction}
  \begin{itemize}
    \item Even though $x$ is a $q\times1$ vector, 
          $x^{T} \beta_0$ is a scalar of a single linear combination, 
          which is called \alert{a single index}.
    \item By the form of the single index model, we obtain
          \begin{align*}
            \mathbb{E}(Y \mid X) = g(X^{T} \beta_0),
          \end{align*}
          which means that 
          the conditional expectation of $Y$ 
          only depends on the vector $X$
          through a single index $X^{T} \beta_0$.
    \item The model is semiparametric 
          when $\beta \in \mathbb{R}^{q}$ is estimated with the parametric methods
          and $g(\cdot)$ with the nonparametric methods.
  \end{itemize}
\end{frame}

\begin{frame}{Examples of Parametric Single Index Model}
  \begin{itemize}
    \item If $g(\cdot)$ is the identity function, 
          then the model turns out to be \alert{a linear regression model}:
          \begin{align*}
            Y = g (X^{T} \beta_0) + u = X^{T} \beta_0 + u.
          \end{align*}
    \item If $g(\cdot)$ is the CDF of Normal$(0, 1)$,
          then the model turns out to be \alert{a probit model}.
          \begin{itemize}
            \item See the textbook for further discussions on a probit model.
          \end{itemize}
    \item If $g(\cdot)$ is the CDF of logistic distribution,
          then the model turns out to be \alert{a logistic regression model}.
  \end{itemize}  
\end{frame}

\begin{frame}{TOC}
\tableofcontents
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Identification Conditions}

\begin{frame}{Identification Conditions}
  \begin{itembox}[l]{Proposition 8.1 (Identification of a Single Index Model)}
    \quad 
    For the semiparametric single index model $Y = g(x^{T} \beta_0) + u$,
    identification of $\beta_0$ and $g(\cdot)$ requires that
    \begin{itemize}
      \item (i)
            $x$ should not contain a constant/an intercept, 
            and must contain at least one continuous variable.
            Moreover, $\|\beta_0\|$=1.
      \item (ii)
            $g(\cdot)$ is differentiable 
            and is not a constant function on the support of $x^{T}\beta_0$.
      \item (iii)
            For the discrete components of $x$, 
            varying the values of the discrete variables will not divide the support of $x^{T}\beta_0$ into disjoint subsets.
    \end{itemize}
  \end{itembox}
\end{frame}

\begin{frame}{Identification Condition (i)}
\begin{itemize}
  \item Note that \alert{the location and the scale of $\beta_0$ are not identified}.
  \item The vector $x$ cannot include an intercept 
        because the function $g(\cdot)$ (which is to be estimated in nonparametric manners) includes any location and level shift.
        \begin{itemize}
          \item That is, $\beta_0$ cannot contain a location parameter.
        \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Identification Condition (i)}
  \begin{itemize}
    \item Some normalization criterion (scale restrictions) for $\beta_0$ are needed.
          \begin{itemize}
            \item One approach is to set $\| \beta_0 \| =1$.
            \item The second approach is to set one component of $\beta_0$ to equal one. 
                  This approach requires that 
                  the variable corresponding to the component set to equal one 
                  is continuously distributed 
                  and has a non-zero coefficient.
            \item Then, $x$ must be dimension $2$ or larger. 
                  If $x$ is one-dimensional, then $\beta_0 \in \mathbb{R}^1$ is simply normalized to 1, 
                  and the model is the one-dimensional nonparametric regression $E(Y \mid x) = g(x)$ with no semiparametric component.
          \end{itemize}
  \end{itemize}
  
\end{frame}

\begin{frame}{Identification Conditions (ii) and (iii)}
\begin{itemize}
  \item The function $g(\cdot)$ cannot be a constant function and must be differentiable on the support of $x^{T}\beta_0$.
  \item $x$ must contain at least one continuously distributed variable
        and this continuous variable must have non-zero coefficient.
        \begin{itemize}
          \item  If not, $x^{T} \beta_0$ only takes a discrete set of values and it would be impossible to identify a continuous function $g(\cdot)$ on this discrete support.
        \end{itemize}
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ichimura's (1993) Method}


\begin{frame}{Ichimura's Method}
\begin{itemize}
  \item Textbook: Sections 8.2; 8.4.1; and 8.12.
  \item Suppose that the functional form of $g(\cdot)$ were known.
  \item Then we could estimate $\beta_0$ by minimizing the least-squares criterion:
        \begin{align*}
        \sum_{i=1} \left[ Y_i - g(X_i^{T}\beta) \right]^2
        \end{align*}
        with respect to $\beta$.
  \item We could think about replacing $g(\cdot)$ with a nonparametric estimator $\hat{g}(\cdot)$.
  \item However, since $g(z)$ is the conditional mean of $Y_i$ given $X_i^{T} \beta_0 = z$,
        \alert{$g(\cdot)$ depends on unknown $\beta_0$}, so we cannot estimate $g(\cdot)$ here.
\end{itemize}
\end{frame}

\begin{frame}{Ichimura's Method}
\begin{itemize}
  \item Nevertheless, \alert{for a fixed value of $\beta$}, we can estimate
        \begin{align*}
        G(X_i^{T} \beta) 
          := \mathbb{E} (Y_i \mid X_i^{T}\beta) 
           = \mathbb{E} (g(X_i^{T}\beta_0) \mid X_i^{T}\beta).
        \end{align*}
  \item In general $G(X_i^{T}\beta) \neq g(X_i^{T} \beta)$.
  \item When $\beta = \beta_0$,
        it holds that $G(X_i^{T}\beta_0) = g(X_i^{T} \beta_0)$.
        \footnote{一般の$X_i^{T}\beta$を用いて条件付けると，$G$と$g$は通常は一致しないが，正しいインデックス$X_i^{T}\beta = X_i^{T}\beta_0$のときだけ一致するということ．}
\end{itemize}  
\end{frame}

\begin{frame}{Ichimura's Method}
  \begin{itemize}
  \item First, we estimate $G(X_i^{T}\beta)$  
        with the leave-one-out NW estimator:
        \begin{align*}
          \hat{G}_{-i}(X_i^{T}\beta) 
              :&= \hat{\mathbb{E}}_{-i}(Y_i \mid X_i^{T} \beta) \\
               &= \frac
                  {\sum_{j \neq i} Y_j 
                   K \left( \frac
                            {X_j^{T}\beta - X_i^{T}\beta}
                            {h} 
                     \right) 
                  }
                  {\sum_{j \neq i} 
                   K \left( \frac
                           {X_j^{T}\beta - X_i^{T}\beta}
                           {h} 
                     \right) 
                  }.
        \end{align*}
\end{itemize}
\end{frame}

\begin{frame}{Ichimura's Method}
  \begin{itemize}
    \item Second, 
          using the leave-one-out NW estimator $\hat{G}_{-i}(X_i^{T}\beta)$,
          we estimate $\beta$ with
          \begin{align*}
            \hat{\beta} 
              &:= \arg \min_{\beta} 
                    \sum_{i=1}^{n}  
                          \left[ Y_i - \hat{G}_{-i}(X_i^{T}\beta) \right]^2 
                          w(X_i) \mathbf{1}(X_i \in A_n) \\
              &:= \arg \min_{\beta} 
                  S_n(\beta),
          \end{align*}
          which is called \alert{Ichimura's estimator (the WSLS estimator)}.
    \item  $w(X_i)$ is a nonnegative weight function.
    \item  $\mathbf{1}(X_i \in A_n)$ is a trimming function to trim out small values of 
           $\hat{p}(X_i^{T}\beta) 
           =
           \dfrac{1}{nh} \sum_{j \neq i} 
              K \left( \frac
                       {X_j^{T}\beta - X_i^{T}\beta}
                       {h} 
                \right) $,
           so that we do not suffer the random denominator problem. 
        \begin{itemize}
          \item $A_\delta = \{ x: p(x^{T}\beta) \geq \delta, \text{ for } ^\forall \beta \in \mathcal{B}\}$.
          \item $A_n = \{ x: || x - x^{\star} || \leq 2h, \text{ for } ^\exists x^{\star} \in A_\delta \}$, 
                which shrinks to $A_\delta$ as $n \to \infty$ and $h \to 0$.
        \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Asymptotic Distribution of Ichimura's Estimator}
  \begin{itemize}
    \item Let $\hat{\beta}$ denote the semiparametric estimator of $\beta_0$
          obtained from minimizing $S_n(\beta)$.
    \item To derive the asymptotic distribution of $\hat{\beta}$, the following conditions are neeeded:
  \end{itemize}
  \end{frame}
  
  
  \begin{frame}{Asymptotic Distribution of Ichimura's Estimator}
    \begin{itembox}[l]{Assumption 8.1}
      The set $A_\delta$ is compact, 
      and the weight function $w(\cdot)$ is bounded and posotive on $A_\delta$.
      Define the set 
      \[D_z = \{ z: z=x^{T}\beta, \beta \in \mathcal{B}, x \in A_\delta \}.\]
      Letting $p(\cdot)$ denote the PDF of $z \in D_z$, 
      $p(\cdot)$ is bounded below by a positive constant for $^\forall z \in D_z$
    \end{itembox}
    \begin{itembox}[l]{Assumption 8.2}
      $g(\cdot)$ and $p(\cdot)$ are 3 times differentiable w.r.t. $z=x^{\beta}$.
      The third derivatives are Lipschitz continuous uniformly over $\mathcal{B}$
      for $^{\forall} z \in D_z$. 
    \end{itembox}
  \end{frame}
  
  \begin{frame}{Asymptotic Distribution of Ichimura's Estimator}
    \begin{itembox}[l]{Assumption 8.3} 
      The kernel function is a bounded second order kernel, which has bounded support;
      is twice differentiable; 
      and its second derivative is Lipschitz continuous.
    \end{itembox}
    \begin{itembox}[l]{Assumption 8.4}
      $\E(|Y^m|) < \infty$ for $^{\exists} m \geq 3$.
      $\var(Y \mid x)$ is bounded and 
      bounded away from zero for $^{\forall} x \in A_\delta$. 
      $\frac{q \ln(h)}{nh^{3 + \frac{3}{m-1}}} \to 0$ and 
      $ nh^8 \to 0$ as $n \to \infty$.
    \end{itembox}
  \end{frame}
  
  \begin{frame}{Asymptotic Distribution of Ichimura's Estimator}
  \textbf{Theorem 8.1.} Under assumptions 8.1 through 8.4,
      \[ \sqrt{n}(\hat{\beta} - \beta_0)
        \darrow \text{Normal} (0, \Omega_I),
      \]
  with
    \begin{align*}
      \Omega_I &= V^{-1} \Sigma V^{-1}, \\
      V &= \E\{
        w(X_i) (g_i^{(1)})^2 \\
        & \times (X_i - E_A(X_i\mid X_i^T \beta_0)) (X_i - E_A(X_i\mid X_i^T \beta_0))^T 
      \}, \\
      \Sigma &= \E \{
        w(X_i) \sigma^2(X_i) (g_i^{(1)})^2 \\
        & \times (X_i - E_A(X_i\mid X_i^T \beta_0)) (X_i - E_A(X_i\mid X_i^T \beta_0))^T 
      \},
    \end{align*}
    where
    \begin{itemize}
      \item $(g_i^{(1)}) = \frac{\partial g(v)}{\partial v}\mid_{v= X_i^T \beta_0}$,
      \item $\E_A (X_i\mid v) = \E(X_i \mid x_A^T\beta_0 = v)$,
      \item $x_A$ has the distribution of $X_i$ conditional on $X_i \in A_\delta$.
    \end{itemize}
  \end{frame}
  
  \begin{frame}{Asymptotic Distribution of Ichimura's Estimator}
  \begin{itemize}
    \item See Ichimura (1993); and Hardle, Hall and Ichimura (1993) for the proof of \textbf{Theorem 8.1}.
    \item Horowitz (2009) provides an excellent heuristic outline for proving \textbf{Theorem 8.1}, 
          using only the familiar Taylor series methods, the standard LLN, and the Lindeberg-Levy CLT.
  \end{itemize}
  \end{frame}
  
  
  \begin{frame}{Optimal Weight under the Homoscedasticity Assumption}
  \begin{itemize}
    \item We introduce the following homoscedasticity assumption:
          \begin{align*}
            \E(u_i^2 \mid X_i) = \sigma^2.
          \end{align*}
    \item Under this assumption, the optimal choice of $w(\cdot)$ is $w(X_i)=1$.
    \item In this case, 
          \[ \hat{\beta} = \arg\min_{\beta} \sum_{i=1}^{n} (Y_i - \hat{G}_{-i}(X_i^{T}\beta)^2)\mathbf{1}(X_i \in A_n) \]
          is \alert{semiparametrically efficient} in the sense that 
          $\Omega_I$ is \alert{the semiparametric variance lower bound} (conditional on $X \in A_\delta$).
  \end{itemize}
  \end{frame}
  
  \begin{frame}{Optimal Weight under Heteroscedasticity}
    \begin{itemize}
      \item In general, $\E(u_i^2 \mid X_i) = \sigma^2(X_i)$.
      \item \alert{An infeasible case}: 
            If one assues that  $\E(u_i^2 \mid X_i) = \sigma^2(X_i^{T}\beta_0)$,
            that is, the conditional variance depends only on the single index $X_i^{T}\beta_0$,
            the choice of $w(X_i) = \frac{1}{\sigma^2({X_i^{T}\beta_0})}$ can lead to a semiparametrically efficient estimation.
      \item We could adopt a two-step procedure as follows.
    \end{itemize}
  \end{frame}
  
  \begin{frame}{Two-Step Procedure to Choose Optimal Weight}
    \begin{itemize}
      \item Suppose that the conditional variance is a function of $X_i^{T}\beta_0$ (Let $\sigma^2({X_i^{T}\beta_0})$ denote it).
      \item \alert{The first step}: Use $w(X_i)=1$ to obtain a $\sqrt{n}$-consistent estimator of $\beta_0$.
        
      \item Let $\tilde{\beta}_0$ denote the estimator of $\beta_0$, 
            and $\tilde{u}_i = Y_i - \hat{g}(X_i^T \tilde{\beta}_0)$ denote the residual obtained from $\tilde{\beta}_0$.
      \item We can obtain a consistent nonparametric estimator of the conditional variance: $\hat{\sigma}^2({X_i^{T} \tilde{\beta}_0})$.
      \end{itemize}
  \end{frame}

  \begin{frame}{Two-Step Procedure to Choose Optimal Weight}
    \begin{itemize}
      \item \alert{The second step}: Estimate $\beta_0$ again using $w(X_i)= \frac{1}{\hat{\sigma}^2({X_i^{T}\tilde{\beta}_0})}$:
            \begin{align*}
              \hat{\beta}_0 = 
              \arg \min_{\beta} 
              \sum_{i=1}^{n}  
                    \left[ Y_i - \hat{G}_{-i}(X_i^{T}\beta) \right]^2 
                    \frac{1}{\hat{\sigma}^2({X_i^{T}\tilde{\beta}_0})} \mathbf{1}(X_i \in A_n).
            \end{align*}
      \item The estimator $\hat{\beta}_0$ is semiparametrically efficient because
            $\hat{\sigma}^2(v) - \sigma^2(v)$ converges to zero at a particular rate 
            uniformly over $v \in D_v$ ($D_v$ is the support of $X_i^{T}\beta_0$).
            \footnote{$\hat{\sigma}^2({X_i^{T}\beta})$を用いるケースもある．}
    \end{itemize} 
  \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Direct Semiparametric Estimators for $\beta$}
  
\begin{frame}{Direct Semiparametric Estimators for $\beta$}
\begin{itemize}
  \item Textbook: Sections 8.3; and 8.4.2.
  \item Here we review:
  \begin{itemize}
    \item Hardle and Stoker's (1989) Average Derivative Estimator, 
    \item Powell, Stock and Stoker's (1989) Density-Weighted Average Derivative Estimator,
    \item Li, Lu and Ullah's (2003) Estimator, and
    \item Hristache, Juditsky and Spokoiny's (2001) Improved Average Derivative Estimator.
  \end{itemize}
  \item The advantage of the direct estimation method is that 
        we can estimate $\beta_0$ and $g(x^T \beta_0)$ directly
        without running the nonlinear least squares, 
        which leads to the computational simplicity.
  \item We still suffer from a finite-sample problem.
\end{itemize}  
\end{frame}

\begin{frame}{Hardle and Stoker's (1989) Average Derivative Estimator}
\begin{itemize}
  \item Suppose that $x$ is a $q \times 1$ vector of continuous variables.
  \item Then we obtain \alert{the average derivative} of $\E(Y \mid X=x)$:
        \begin{align*}
          \E \left[ 
            \frac{\partial \E(Y \mid X=x)}{\partial x}
          \right]
          =
          \E \left[
            g^{(1)}(x^T \beta_0)
          \right]
          \beta_0
        \end{align*}
  \item Recall that the scale of $\beta_0$ is not identified, 
        which means that the constant 
        $\E \left[ g^{(1)}(x^T \beta_0)\right]$ 
        does not matter.
        That is, a normalized estimation of 
        $\E \left[ \frac{\partial \E(Y \mid X=x)}{\partial x}\right]$
        is an estimation of normalized $\beta_0$.
\end{itemize}
\end{frame}

\begin{frame}{Hardle and Stoker's (1989) Average Derivative Estimator}
\begin{itemize}
  \item Let $\hat{\E}(Y_i \mid X_i)$ denote the NW estimator of $\E(Y_i \mid X_i)$:
        \begin{align*}
          \hat{\E}(Y_i \mid X_i) 
          = \frac
            {\sum_{j=1}^{n} Y_j K\left( \frac{X_i - X_j}{a} \right)}
            {\sum_{j=1}^{n} K\left( \frac{X_i - X_j}{a} \right)}.
        \end{align*}
  \item Assuming that the kernel function is differentiable, 
        we can estimate $\beta_0$, estimating 
        $\E \left[ 
          \frac{\partial \E(Y \mid X=x)}{\partial x}
        \right]$
        with its sample analogue:
        \begin{align*}
          \tilde{\beta}_{ave} 
          = \frac{1}{n} \sum_{i=1}^{n}
            \frac{\partial \hat{\E}(Y_i \mid X_i)}{\partial X_i}.
        \end{align*}
  \item The scale normalization can also be implemented by 
        $\dfrac{\tilde{\beta}_{ave}}{|\tilde{\beta}_{ave}|}$.
\end{itemize}  
\end{frame}

\begin{frame}{Hardle and Stoker's (1989) Average Derivative Estimator}
\begin{itemize}
  \item An issue raised with this estimator is \alert{the random denominator problem},
        which leads to a difficulty in the derivation of the asymptotic properties.
  \item Rilstone (1991) establishes the $\sqrt{n}$-normality using a trimming function.
\end{itemize}  
\end{frame}


\begin{frame}{Powell, Stock and Stoker's (1989) Density-Weighted Average Derivative Estimator}
\begin{itemize}
  \item As we obtain the average derivative above, 
        we also obtain \alert{the weighted average derivative} of $\E(Y \mid X=x)$:
        \begin{align*}
          \E \left[ w(x)
            \frac {\partial \E(Y \mid X=x)}{\partial x}
          \right]
          =
          \E \left[
            w(x) g^{(1)}(x^T \beta_0)
          \right]
          \beta_0.
        \end{align*}
    
\end{itemize}
\end{frame}



\begin{frame}{Powell, Stock and Stoker's (1989) Density-Weighted Average Derivative Estimator}
  \begin{itemize}
    \item Let $w(x)$ be the density function $f(x)$, 
          and $\delta$ denote \alert{the density-weighted average derivative} of $\E(Y \mid X=x)$.
    \item Then we obtain    
          \begin{align*}
          \delta &= \E \left[ f(X)
                          \frac {\partial \E(Y \mid X=x)}{\partial x}
                        \right] \\
                 &= \E \left[ f(X)
                          g^{(1)}(X^T \beta_0)
                        \right] \\
                 &= \int  g^{(1)}(x^T \beta_0) f^2(x) dx\\
                 &= g(x^T \beta_0)f^2(x)
                    - 2 \int g(x^T \beta_0) f^{(1)}(x) f(x) dx.
            \end{align*}   
  \end{itemize}
\end{frame}

\begin{frame}{Powell, Stock and Stoker's (1989) Density-Weighted Average Derivative Estimator}
\begin{itemize}
  \item Assume that $f(x)=0$ at the boundary of the support of $X$.
        Then we observe that $g(x^T \beta_0)f^2(x)= 0$, that is,
        \begin{align*}
        \delta &= - 2 \int g(x^T \beta_0) f^{(1)}(x) f(x) dx \\
               &= -2 \E [g(X^{T}\beta_0)f^{(1)}(X)] \\
               &= -2 \E [Y f^{(1)}(X)].
        \end{align*}    
  \item We can estimate $\delta$ by its sample analogue:
        \begin{align*}
          \hat{\delta} = - \frac{2}{n} \sum_{i=1}^{n} Y_i \hat{f}_{-i}^{(1)}(X_i),
        \end{align*}
        where $\hat{f}_{-i}(X_i)$ is the leave-one-out NW estimator of $f(X)$:
        \begin{align*}
          \hat{f}_{-i}(X_i) = \frac{1}{n-1} \sum_{j \neq i} \left(\frac{1}{h}\right)^q K \left( \frac{X_i - X_j}{h} \right).
        \end{align*}
\end{itemize}
\end{frame}

\begin{frame}{Powell, Stock and Stoker's (1989) Density-Weighted Average Derivative Estimator}
\begin{itemize}
  \item There is \alert{no denominator messing with uniform convergence}. 
        There is only a density estimator, no conditional mean needed.
  \item The textbook uses the NW estimator $\hat{f}^{(1)}(X_i)$ in (8.17). 
  \item However, \alert{Powell, Stock and Stoker (1989) define their estimator 
        using the leave-one-out NW estimator $\hat{f}_{-i}^{(1)}(X_i)$}.
  \item Here we proceed with Powell, Stock and Stoker (1989).
\end{itemize}
\end{frame}
  
  
\begin{frame}{Powell, Stock and Stoker's (1989) Density-Weighted Average Derivative Estimator}
\begin{itemize}
  \item A useful representation of $\hat{\delta}$ is given by 
        \begin{align*}
          \hat{\delta} 
            = \frac{-2}{n(n-1)}
              \sum_{i=1}^{n} \sum_{j \neq i}
              \left( \frac{1}{h} \right)^{q+1}
              Y_i K^{(1)} \left( \frac{X_i - X_j}{h} \right).
        \end{align*}
  \item Under some assumptions, 
        if $h \to 0$ and $nh^{q+2} \to \infty$ hold,
        then the density-weighted average derivative estimator $\hat{\delta}$ satisfies that 
        \begin{align*}
          \sqrt{n} (\hat{\delta} - \E[\hat{\delta}]) \darrow \text{Normal}(0, \Sigma_{\delta}),
        \end{align*}
        where $\Sigma_{\delta} = 4\E[\sigma^2(X) f^{(1)}(X) f^{(1)}(X)^T] + 4 \Var( f(X) g^{(1)}(X))$. 
\end{itemize}  
\end{frame}

\begin{frame}{$U$-Statistics Form of $\hat{\delta}$}
 \begin{itemize}
  \item Recall that $K(\cdot)$ is differentiable and symmetric, that is,
        $K^{(1)}(u) = -K^{(1)}(-u)$. Then, we obtain the standard $U$-statistics form of $\hat{\delta}$:
        \begin{align*}
          \hat{\delta} 
          =
          - \begin{pmatrix}  
            n \\ 2
          \end{pmatrix}^{-1}
          \sum_{i=1}^{n-1} \sum_{j=i+1}^{n}
          \left( \frac{1}{h} \right)^{q+1}
          K^{(1)} \left( \frac{X_i - X_j}{h} \right)
          (Y_i - Y_j).
        \end{align*}
  \item Letting $Z_i$ denote $(Y_i, X_i^T)^T$ 
        and $p_n(Z_i, Z_j)$ denote $-\frac{1}{h^{q+1}}K^{(1)} \left( \frac{X_i - X_j}{h} \right)(Y_i - Y_j)$,
        $\hat{\delta}$ can be rewritten as
        \begin{align*}
          \hat{\delta}
          =
          \begin{pmatrix}  
            n \\ 2
          \end{pmatrix}^{-1}
          \sum_{i=1}^{n-1} \sum_{j=i+1}^{n}
          p_n(Z_i, Z_j).
        \end{align*}
  \item This representation of $\hat{\delta}$ permits a direct analysis of its asymptotic properties, 
        based on the asymptotic theory of $U$-statistics.
        Further discussions can be seen in Serfling (1980); van der Vaart (2000, Chapter 12).
 \end{itemize} 
\end{frame}

\begin{frame}{Powell, Stock and Stoker's (1989) Density-Weighted Average Derivative Estimator}
\begin{itemize}
  \item The asymptotic bias is a bit complicated.
  \item Let $q$ be the dimension of $X$, and set 
        \begin{align*}
          p = \left\{
            \begin{array}{l}
              \dfrac{q+4}{2} \quad \text{ if } q \text{ is even}, 
              \\ \\
              \dfrac{q+3}{2} \quad \text{ if } q \text{ is odd}.
            \end{array}
            \right.
        \end{align*}
  \item The kernel function $K(\cdot)$ for the estimation of $f(\cdot)$ is required to be of order at least $p$.
  \item The asymptotic bias is 
        $\sqrt{n}(\E(\hat{\delta}) - \delta) = O(n^{\frac{1}{2}}h^p)$,
        which is $o(1)$ if $nh^{2p} \to 0$.
\end{itemize}  
\end{frame}

\begin{frame}{Powell, Stock and Stoker's (1989) Density-Weighted Average Derivative Estimator}
\begin{itemize}
  \item $nh^{2p} \to 0$ is violated if $h$ is selected to be optimal for the estimation of $f(\cdot)$ or $f^{(1)}(\cdot)$.
        That is, this requirement needs \alert{the bandwidth $h$ to undersmooth to reduce the bias}.
        Further discussions on the bandwidth selection follow in Section 8.4.
  \item Cattaneo, Crump and Jansson (2010, 2011) 
        introduce another asymptotic theory to relax strong assumptions .
  \item Nishiyama and Robinson (2005): Density-weighted average derivative estimators can be refined by bootstrapping methods.
\end{itemize}  
\end{frame}

\begin{frame}{Nishiyama and Robinson (2005)}
  \begin{figure}
    \begin{center}
    \includegraphics[width=1.0\textwidth]{material/Nishiyama_Robinson_2005_abst.png}
    \end{center}
  \end{figure}
\end{frame}

\begin{frame}{Li, Lu and Ullah's (2003) Estimator}
\begin{itemize}
  \item We consider the estimation of the average derivative $\E[g^{(1)}(X^T\beta_0)]$ again.
  \item We can also use the local polynomial method for the estimation of $g^{(1)}(X^T\beta_0)$.
  \item Let $\hat{g}^{(1)}(X_i^T\beta_0)$ denote the kernel estimator of $g^{(1)}(X_i\beta_0)$,
        which is obtained from an $m$-th order local polynomial regression.
  \item Li, Lu and Ullah (2003) suggest to use $\tilde{\beta}_{ave} = \dfrac{1}{n}\hat{g}^{(1)}(X_i^T\beta_0)$ 
        to estimate $\beta = \E[{g}^{(1)}(X^T\beta_0)]$.
\end{itemize}
\end{frame}

\begin{frame}{Li, Lu and Ullah's (2003) Estimator}
\begin{itemize}
  \item Their approach does not require the condition $f(x)=0$ at the boundary of the support of $X$. 
        However, they require to assume that 
        \begin{itemize}
          \item the support of $X$ is a compact set, and that 
          \item the density $f(x)$ is bounded below by a positive constant at the support of $X$,
        \end{itemize}
        which leads to avoiding the use of a trimming function. 
\end{itemize}  
\end{frame}

\begin{frame}{Li, Lu and Ullah's (2003) Estimator}
\begin{itemize}
  \item Under the assumptions so far and some additional conditions, 
        if we use a second order kernel, 
        where $n\displaystyle\sum_{s=1}^{q} a_s^{2m} \to 0$ 
        and $\dfrac{n a_1 \cdots a_q \sum_{s=1}^{q}}{\ln (n)} \to \infty$ 
        with $m$ denoting the order of local polynomial estimation, then,
        \begin{align*}
          \sqrt{n} (\tilde{\beta}_{ave} - \beta) 
          \darrow \text{Normal}\left(0, \Phi + \var(g^{(1)}(X^{T}\beta_0)) \right),
        \end{align*} 
        where $\Phi = \E \left[ \dfrac{\sigma^2(X) f^{(1)}(X) f^{(1)}(X)^T}{f^{(2)}(X)} \right]$.
  \item The proof of the asymptotic normality can be derived from $U$-statistics theory.
  \item Newey (1994) shows that the asymptotic variance does not depend on the specific nonparametric estimation method.
\end{itemize}  
\end{frame}

\begin{frame}{Hristache, Juditsky and Spokoiny's (2001) Improved Average Derivative Estimator}
\begin{itemize}
  \item Powell, Stock and Stoker's (1989) density-weighted average derivative estimator requires 
        the density of $X$ to be increasingly smooth as the dimension of $X$ increases.
  \item This is necessary to make $\sqrt{n}(\hat{\delta} - \delta)$ asymptotically normal with a mean of 0.
  \item \alert{Practical Consequence}: The finite-sample performance of the density-weighted average derivative estimator
        is likely to be deteriorated as the dimension of $X$ increases, 
        especially if the density of $X$ is not very smooth.
  \item Specifically, the estimator's bias and MSE are likely to increase as the the dimension of $X$ increases.
\end{itemize}
\end{frame}


\begin{frame}{Hristache, Juditsky and Spokoiny's (2001) Improved Average Derivative Estimator}
\begin{itemize}
  \item Hristache, Juditsky and Spokoiny (2001) introduce an iterated average derivative estimator that overcomes this problem.
  \item Their estimator is based on the observation that $g(x^T\beta_0)$ does not vary 
        when $x$ varies in a direction that is orthogonal to $\beta_0$.
  \item Therefore, only the directional derivative of $\E(Y \mid X = x)$ along the direction of $\beta$ is needed for estimation.
  \item Suppose that this direction were known. 
        Then estimating the directional derivative would be a one-dimensional nonparametric estimation problem,
        and there would be no curse of dimensionality.
\end{itemize}
\end{frame}

\begin{frame}{Hristache, Juditsky and Spokoiny's (2001) Improved Average Derivative Estimator}
\begin{itemize}
  \item In practice, the direction of $\beta$ is unknown.
  \item Hristache, Juditsky and Spokoiny (2001) show that this can be estimated with sufficient accuracy 
        through \alert{an iterative procedure}.
  \item Their idea is to use prior information about the vector $\beta$
        for improving the quality of the gradient estimate 
        by extending a weighting kernel in the direction of small directional derivatives,
        and they demonstrate that the whole procedure requires at most $2 \log (n)$ iterations.
  \item Under relatively mild assumptions, their estimator is $\sqrt{n}$-consistent.
  \item See Horowitz (2009, Section 2.6) for further discussions.
\end{itemize}
\end{frame}

\begin{frame}{Estimation of $g(\cdot)$}
\begin{itemize}
  \item Let $\beta_n$ denote a $\sqrt{n}$-consistent estimator of $\beta$, or $\delta$.
  \item Once we obtain $\beta_n$, we can estimate $g(x^T\beta_0)$ by 
        \begin{align*}
          \hat{g}(x^T\beta_n) = 
          \frac
          {\sum_{j=1}^{n} Y_j K \left( \frac{(X_j - x)^T \beta_n}{h} \right)}
          {\sum_{j=1}^{n} K \left( \frac{(X_j - x)^T \beta_n}{h} \right)}.
        \end{align*}
  \item Recall that $\beta_n$ is a $\sqrt{n}$-consistent estimator of $\beta$, that is,
        $\beta_n - \beta_0 = Op(n^{-\frac{1}{2}})$, 
  \item This converges to zero faster than standard nonparametric estimators.
  \item Then, the asymptotic distribution of $\hat{g}(x^T\beta_0)$ is the same as that of $\hat{g}(x^T\beta_n)$.
  \item Thus, we obtain \textbf{Corollary 8.1}:
        \begin{align*}
          \sqrt{nh} [\hat{g}(x^T\beta_n) - g(x^T\beta_0) - h^2 B(x^\beta_0)] 
          \darrow 
          \text{Normal}\left( 0, \frac{\kappa \sigma^2(x^T\beta_0)}{f(x^T\beta_0)} \right).
        \end{align*}
\end{itemize}
\end{frame}


\begin{frame}{Generalized Cases?}
\begin{itemize}
  \item The direct average derivative estimation method discussed previously
        is applicable only when $x$ is a $q \times 1$ vector of continuous variables
        because the derivative w.r.t. discrete variables is not defined.
  \item Horowitz and Hardle (1996) discuss how direct (noniterative) estimation can be generalized to cases
        for which some components of $x$ are discrete.
        Horowitz (2009) provides an excellent overview of this method. 
\end{itemize}
\end{frame}

\begin{frame}{Finite-Sample Problem}
\begin{itemize}
  \item Nonparametric estimation in the 1st stage may suffer from the curse of dimensionality.
  \item In small-sample settings, the iterative method of Ichimura (1993) may be more appealing 
        as it avoids having to conduct high-dimensional nonparametric estimation.
\end{itemize}
\end{frame}

\begin{frame}{Carroll, Fan, Gijbels and Wand (1997)}
\begin{itemize}
  \item They consider the problem of estimating a general partially linear single index model 
        which contains both a partially linear model and a single index model as special cases.
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bandwidth Selection}
  
\begin{frame}{Bandwidth Selection for Ichimura's Estimator}
  \begin{itemize}
    \item Recall that we assume in Assumption 8.4 that 
          $\dfrac{q \ln(h)}{nh^{3 + \frac{3}{m-1}}} \to 0$ and 
          $ nh^8 \to 0$ as $n \to \infty$, 
          where $m \geq 3$ is a positive integer 
          whose specific value depends on the existence of the number of finite moments of $Y$ 
          along with the smoothness of the unknown function $g(\cdot)$.
          \footnote{Assumption 8.4は，$g$をノンパラメトリックに推定することがパラメトリックパートの収束レートに影響を与えないための十分条件になっている．}
    \item The range of permissive smoothing parameters allows for optimal smoothing, i.e., 
          $h=O(n^{-\frac{1}{5}})$.
          \footnote{このオーダーで選んだ$h$は，Assumption 8.4を満たしている．}
  \end{itemize}
\end{frame}

\begin{frame}{Bandwidth Selection for Ichimura's Estimator}
  \begin{itemize}
    \item Our aim is to choose $\hat{\beta}$ close to $\beta_0$, 
          and $h$ close to the value $h_0$, 
          which minimize the average of 
          \begin{align*}
            \E\{ \hat{g}(X_i^{T}\beta_0 \mid X_i^{T}\beta_0) - g(X_i^{T}\beta_0) \}^2.
          \end{align*}
    \item Hardle, Hall and Ichimura (1993) suggest 
          picking $\beta$ and the bandwidth $h$ jointly
          by minimization of $S_n(\beta)$.
  \end{itemize}    
\end{frame}
  
\begin{frame}{Bandwidth Selection for Ichimura's Estimator}
\begin{itemize}
  \item Recall the proof of \textbf{Theorem 8.1}. 
        We have established the following decomposition of the least squares criterion:
        \begin{align*}
          S_n(\beta, h) 
                     &= \frac{1}{n} \sum_{i=1}^{n}(Y_i \hat{G}_{-i}(X_i^T\beta))^2 \\
                     &= \frac{1}{n} \sum_{i=1}^{n}(Y_i-G(X_i^T\beta))^2 \\
                     &+ \frac{1}{n} \sum_{i=1}^{n}({G}_{-i}(X_i^T\beta_0)-g(X_i\beta_0))^2 + op(1) \\
                     &\equiv S(\beta) + T(h) + op(1).
        \end{align*}
  \item Minimizing $S_n(\beta, h)$ simultaneously over both 
        $(\beta, h) \in \mathcal{B}_n \times \mathcal{H}_n$ is equivalent to 
        \begin{itemize}
          \item first minimizing $S(\beta)$ over $\beta \in \mathcal{B}_n$; and
          \item second minimizing $T(h)$ over $h \in \mathcal{H}_n$. 
        \end{itemize}
\end{itemize}
\end{frame}
  
\begin{frame}{Bandwidth Selection for Ichimura's Estimator}
\begin{itemize}
  \item Let $(\hat{\beta},\hat{h})$ be the minimizers of $S_n(\beta, h)$.
  \item Suppose that we use the second order kernel. 
        Hardle, Hall and Ichimura (1993) show that the MSE optimal bandwidth satisfies $\hat{h} = O(n^{-\frac{1}{5}})$, and $\dfrac{\hat{h}}{h_0} \parrow 1$.
\end{itemize}  
\end{frame}

\begin{frame}{Bandwidth Selection for Ichimura's Estimator}
  \begin{itemize}
    \item Compare the regularity conditions used in Ichimura (1993) with those in Hardle, Hall and Ichimura (1993).
  \end{itemize}
  \begin{minipage}{0.49\textwidth}  % [t]を指定して上揃え
    \begin{tcolorbox}[colframe=red!50!white, colback=red!10!white, title=Ichimura (1993)]
        \begin{itemize}
          \item A second order kernel is used.
          \item $h$ satisfies assumption 8.4.
          \item $\E[|Y^m|]<\infty$ for $^\exists m \geq 3$.
        \end{itemize}
    \end{tcolorbox}
  \end{minipage}%
  \hfill
  \begin{minipage}{0.49\textwidth}  % [t]を指定して上揃え
    \begin{tcolorbox}[colframe=red!50!white, colback=red!10!white, title=HHI (1993)]
        \begin{itemize}
            \item A higher order kernel is needed.
            \item $h = O(n^{-\frac{1}{5}})$.
            \item $Y$ has moments of any order.
        \end{itemize}
    \end{tcolorbox}
  \end{minipage} 
  
\end{frame}

\begin{frame}{Bandwidth Selection for Average Derivative Estimator}
\begin{itemize}
  \item The estimation of $\beta_0$ involves 
        the $q$-dimensional multivariate nonparametric estimation of the first order derivatives.
  \item \alert{Smoothing Parameters for $\hat{f}_{-i}^{(1)}(X_i)$}: 
        Hardle and Tsybakov (1993) suggest 
        to choose the smoothing parameters $h_1, \cdots, h_q$ 
        to minimize MSE of $\hat{\delta}$.
  \item They show that the asymptotically optimal bandwidth is given by 
        $h_s = c_s n^{-\frac{2}{2q+v+2}}, \text{ for all } s = 1, \dots, q$, 
        where $c_s$ is the constant, and $v$ is the order of kernel.
  \item Powell and Stoker (1996) provide a method for estimating $c_s$.
  \item Horowitz (2009) suggests to select $h_s$ based on bootstrap resampling.
\end{itemize}
\end{frame}

\begin{frame}{Bandwidth Selection for Average Derivative Estimator}
\begin{itemize}
  \item \alert{Smoothing Parameters for $\hat{g}({X_i}^T\beta_n)$}: 
        Once we select the optimal $h_s$'s, we can obtain an estimator of $\beta$.
        Let $\beta_n$ denote a generic estimator.
  \item We estimate $\E[y|x] = g(x^T\beta_0)$ by $\hat{g}(x^T\beta_n, h) = \hat{g}(x^T\beta_n)$.
        The smoothing parameter associated with the scalar index $x^T \beta_n$ can be selected by least squares cross-validation:
        \begin{align*}
          \hat{h} = \arg\min_{h} \sum_{i=1}^{n} [Y_i - \hat{g}_{-i}(X_i^T\beta_n, h)]^2.
        \end{align*}
  \item Under some regularity conditions, the selection of $h$ is of order $O(n^{-\frac{1}{5}})$.
\end{itemize}
\end{frame}

 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
\section{Klein and Spady's (1993) Estimator}
  
\begin{frame}{Semiparametric Binary Choice Model}
 \begin{itemize}
  \item Consider the following binary choice model
        \footnote{(8.2)式を参照せよと書いてあるが，$\beta$の識別のためには定数項の係数$\alpha$を抜いた方がよいのでは？}
        :
        \begin{align*}
          Y_i = \left\{
            \begin{array}{l}
              1 \text{ if } Y_i^{\star} = \alpha + X_i^T \beta + \epsilon_i > 0, 
              \\ 
              0 \text{ if } Y_i^{\star} = \alpha + X_i^T \beta + \epsilon_i \leq 0.
            \end{array}
            \right.
        \end{align*}
  \item This model can be rewritten as
        \begin{align*}
          \E(Y_i \mid X_i) 
          &= \P(Y_i = 1 \mid X_i)\\
          &= \P( \alpha + X_i^T \beta + \epsilon_i > 0)\\
          &= \P(\epsilon_i > -X_i^T \beta - \alpha)
          \equiv g(X_i^T\beta),
        \end{align*}
        which means that the binary choice model is a special case of the single index models.
 \end{itemize} 
\end{frame} 
  
  
\begin{frame}{Semiparametric Binary Choice Model}
\begin{itemize}
  \item Suppose that $g(\cdot)$ were known. 
        We would estimate $\beta$ by maximum likelihood methods.
        The likelihood function would be 
        \begin{align*}
          L^{\star}(b) &= \P(\epsilon > -X_i^Tb - \alpha)^{\sum_{i=1}^n Y_i}\\
                       &\times 
                        \P(\epsilon \leq -X_i^Tb  -\alpha)^{\sum_{i=1}^n (1-Y_i)} \\
                       &= g(X_i^Tb)^{\sum_{i=1}^n Y_i} \times \{1-g(X_i^Tb)\}^{\sum_{i=1}^n (1-Y_i)},
        \end{align*}
        and then the log-likelihood function would be
        \begin{align*}
          L(b) = \sum_{i=1}^{n}[Y_i \log g(X_i^Tb) + (1 - Y_i) \log (1 - g(X_i^Tb))].
        \end{align*}
\end{itemize}
\end{frame}  
 
\begin{frame}{Klein and Spady's (1993) Binary Choice Estimator}
 \begin{itemize}
  \item In reality, $g(\cdot)$ is unknown.
  \item Klein and Spady (1993) suggest to replace $g(\cdot)$ with the leave-one-out NW estimator
        $\hat{g}_{-i}(X_i^T \beta) = \frac{\sum_{j \neq i} K \left( \frac{(X_i - X_j)^T \beta}{h}\right)Y_j}{\sum_{j \neq i} K \left( \frac{(X_i - X_j)^T \beta}{h}\right)}.$
  \item Making this substitution, and adding a trimming function, this leads to the feasible likelihood criterion:
        \begin{align*}
          L(\beta) = \sum_{i=1}^{n}[Y_i \log \hat{g}_{-i}(X_i^T \beta) + (1 - Y_i) \log (1 - \hat{g}_{-i}(X_i^T \beta))]1_{i}(b),
        \end{align*}
        where the trimming indicator should not be a function of $\beta$, but instead of a preliminary estimator:
        \begin{align*}
          1_i(b) = 1\left( \hat{f}_{X^T\tilde{\beta}}(X_i^T\tilde{\beta}) \geq b \right),
        \end{align*}
        with a preliminary estimator $\tilde{\beta}$ and density estimator $\hat{f}_{X^T\tilde{\beta}}(\cdot)$.
 \end{itemize} 
\end{frame}

\begin{frame}{Asymptotic Properties of Klein and Spady's Estimator}
\begin{itemize}
  \item The following asymptotic properties hold 
        \begin{itemize}
          \item under some regularity conditions, and
          \item assuming that the kernel $k$ is of higher-order (must be of fourth-order).
        \end{itemize}
  \item Define $G(X_i^T \beta) = \E[g(X_i^T\beta_0)\mid X_i^T\beta]$. 
        Then we obtain the asymptotic distribution:
        \begin{align*}
          \sqrt{n}(\hat{\beta} - \beta) \darrow \text{Normal}(0, \Omega),
        \end{align*}
        where the asymptotic variance is given by 
        \begin{align*}
          \Omega = \E \left[ 
            \frac{\partial}{\partial\beta}G(X_i^T\beta)
            \frac{\partial}{\partial\beta}G(X_i^T\beta)^T
            \frac{1}{g(X_i^T\beta_0)(1-g(X_i^T\beta_0))}
          \right]^{-1}.
        \end{align*}
  \item Klein and Spady's (1993) estimator achieves \alert{the semiparametric efficiency bound for the single-index binary choice model} (not for the general single-index model).
\end{itemize}
\end{frame}

\begin{frame}{}
  
\end{frame}


  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Lewbel's (2000) Estimator}
  
  
  
  
  
  















  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
\section{Manski's (1975) Maximum Score Estimator}
  
  
  
  
  
  
  


  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
\section{Horowitz's (1992) Smoothed Maximum Score Estimator}
  
  
  
  
  
  
  
  
  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Han's (1987) Maximum Rank Estimator}
  
  
  



  
  
  
  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multinomial Discrete Choice Models}
  
  
  
  
  
  
  



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ai's (1997) Semiparametric Maximum Likelihood Approach}
  
  
  
  
  
  


  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\section{References}
  


\begin{frame}{References}
\begin{itemize}
  \item Carroll, R. J., J. Fan, I. Gijbels, and M. P. Wand (1997)
        ``Generalized Partially Linear Single-Index Models,'' 
        \textit{Journal of the American Statistical Association,} 92(438), 477-489.
  \item Cattaneo, M. D., R. K. Rcump, and M. Jansson (2010)
        ``Robust Data-Driven Inference for Density-Weighted Average Derivatives,''
        \textit{Journal of the American Statistical Association,} 105, 1070-1083.
  \item Cattaneo, M. D., R. K. Rcump, and M. Jansson (2014)
        ``Small Bandwidth Asymptotics for Density-Weighted Average Derivatives,''
        \textit{Econometric Theory,} 30(1), 176-200.
  \item Hardle, W., P. Hall, and H. Ichimura (1993)
        ``Optimal Smoothing in Single-Index Models,'' 
        \textit{Annals of Statistics,} 21, 157-178.
\end{itemize}
\end{frame}

\begin{frame}{References}
\begin{itemize}
  \item Hardle, W., and T. M. Stoker (1989)
        ``Investigating Smooth Multiple Regression by the Method of Average Derivatives,''
        \textit{Journal of the American Statistical Association,} 84(408), 986-995.
  \item Hardle, W., and A. B. Tsybakov (1993)
        ``How Sensitive are Average Derivatives?''
        \textit{Journal of Econometrics,} 58, 31-48.
  \item Horowitz, J. L., and W. Hardle (1996)
        ``Direct Semiparametric Estimation of Single-Index Models with Discrete Covariates,''
        \textit{Journal of the American Statistical Association,} 91(436), 1632-1640.
  \item Hristache, M., A. Juditsky, and V. Spokoiny (2001)
        ``Direct Estimation of the Index Coefficient in a Single-Index Model,''
        \textit{The Annals of Statistics,} 29(3), 593-623.
\end{itemize}
\end{frame}

\begin{frame}{References}
 \begin{itemize}
  \item Ichimura, H. (1993) 
        ``Semiparametric Least Squares (SLS) and Weighted SLS Estimation of Single-Index Models,''
        \textit{Journal of Econometrics,} 3, 205-228. 
  \item Li, Q., X. Lu, and A. Ullah. (2003)
        ``Multivariate Local Polynomial Regression for Estimating Average Derivatives,''
        \textit{Journal of Nonparametric Statistics,} 15(4-5), 607-624.
  \item Liang, H., and N. S. Wang (2005) 
        ``Partially Linear Single-Index Measurement Error Models,'' 
        \textit{Statistica Sinica,} 15, 99-116.
  \item Newey, W. K. (1994)
        ``Kernel Estimation of Partial Means and a General Variance Estimator,''
        \textit{Econometric Theory,} 10, 233-253.
  \item Nishiyama, Y., and P. M. Robinson (2005)
        ``The Bootstrap and the Edgeworth Correction for Semiparametric Averaged Derivatives,''
        \textit{Econometrica,} 73, 903-948.
 \end{itemize} 
\end{frame}

\begin{frame}{References}
  \begin{itemize}
  \item Powell, J. L., J. H. Stock, and T. M. Stoker (1989)
        ``Semiparametric Estimation of Index Coefficients,''
        \textit{Econometrica,} 57(6), 1403-1430. 
  \item Powell, J. L., and T. M. Stoker (1996)
        ``Optimal Bandwidth Choice for Density-Weighted Averages,''
        \textit{Journal of Econometrics,} 75(2), 291-316.
  \item Rilstone, P. (1991)
        ``Nonparametric Hypothesis Testing with Parametric Rates of Convergence,''
        \textit{International Economic Review,} 32(1), 209-227.
  \item Xia, Y., H. Tong, and W. K. Li (1999)
        ``On Extended Partially Linear Single-Index Models,''
        \textit{Biometrika,} 86(4), 831-842.
  \end{itemize}
\end{frame}

\begin{frame}{References}
  \begin{itemize}
    \item Textbooks and Lecture Notes:
    \begin{itemize}
      \item Horowitz, J. L. (2009)
            \textit{Semiparametric and Nonparametric Methods in Econometrics,}
            Springer.
      \item Li, Q., and J. S. Racine (2007). 
            \textit{Nonparametric Econometrics: Theory and Practice,} 
            Princeton University Press.
      \item Serfling, R. J. (1980)
            \textit{Approximation Theorems of Mathematical statistics.} Wiley.
      \item van der Vaart, A. W. (2000)
            \textit{Asymptotic Statistics.} Cambridge.
      \item 末石直也 (2024) 『データ駆動型回帰分析：計量経済学と機械学習の融合』日本評論社．
      \item 西山慶彦，人見光太郎 (2023) 『ノン・セミパラメトリック統計解析（理論統計学教程：数理統計の枠組み）』共立出版．
      \item ECON 718 NonParametric Econometrics (Bruce Hansen, Spring 2009, University of Wisconsin-Madison).
      \item セミノンパラメトリック計量分析（末石直也，2014年度後期，京都大学）．
    \end{itemize}
  \end{itemize}
\end{frame}




\end{document}