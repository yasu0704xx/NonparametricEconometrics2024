\documentclass[xcolor=svgnames,dvipdfmx,cjk]{beamer} 
\AtBeginDvi{\special{pdf:tounicode 90ms-RKSJ-UCS2}} 
\usetheme[progressbar=frametitle]{metropolis}
\setbeamertemplate{headline}[miniframes]
\setbeamercolor{background canvas}{bg=White} 
\setbeamercolor{frametitle}{fg=white, bg=gray!40!black}  
\setbeamercolor{section in head/foot}{fg=pink!70!red, bg=gray!30!black}
\setbeamercolor{alerted text}{fg=orange!50!red} 
\usefonttheme{professionalfonts}
\setbeamertemplate{theorems}[numbered]
\newtheorem{thm}{Theorem}[section]
\newtheorem{proposition}[thm]{Proposition}
\theoremstyle{example}
\newtheorem{exam}[thm]{Example}
\newtheorem{remark}[thm]{Remark}
\newtheorem{question}[thm]{Question}
\newtheorem{prob}[thm]{Problem}
\def\Avar{\text{Avar}}
\def\var{\text{var}}
\def\Var{\text{Var}}
\def\cov{\text{cov}}
\def\Cov{\text{Cov}}
\def\E{\mathbb{E}}
\def\H{\mathbb{H}}
\def\R{\mathbb{R}}
\def\P{\mathbb{P}}
\def\parrow{\xrightarrow{p}}
\def\darrow{\xrightarrow{d}}
\def\plim{\text{plim }}
\usepackage{bbm}
\usepackage{ascmac}
\usepackage{tcolorbox}
\usepackage{tabularray}
\usepackage{float}
\usepackage{graphicx}
\usepackage{codehigh}
\usepackage[normalem]{ulem}
\UseTblrLibrary{booktabs}
\UseTblrLibrary{siunitx}
\newcommand{\tinytableTabularrayUnderline}[1]{\underline{#1}}
\newcommand{\tinytableTabularrayStrikeout}[1]{\sout{#1}}
\NewTableCommand{\tinytableDefineColor}[3]{\definecolor{#1}{#2}{#3}}

\begin{document} 

%%%%%講演に関する情報%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Selectivity Models} 
\subtitle{Li and Racine (2007, Chapter 10)}
\author{Yasuyuki Matsumura}
\institute{Graduate School of Economics, Kyoto University}
\date{\today} % 日付を自動で挿入

%%%%%タイトルページ%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}                  
\titlepage            
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Before showwing the contents of the slides...
\section{Sample Selection Issues}

\begin{frame}{Sample Selection}

      \begin{itemize}
            \item Sample selection issues frequently arise in empirical studies.
            \item We concern that the treatment effect for those ``selected as treated'' 
                  will differ from that for persons randomly selected from the general population.
            \item Pioneering parametric approaches to deal with sample selection can be found in Heckman (1976, 1979).
      \end{itemize}
      
\end{frame}


\section{Semiparametric Type-2 Tobit Models}

\begin{frame}{Type-2 Tobit Models}
      \begin{itemize}
            \item The Type-2 Tobit model is the four equation system:
                  \begin{align*}
                        Y_{1i}^{\star} &= X_{1i}^T \beta_1 + u_{1i},\\
                        Y_{2i}^{\star} &= X_{2i}^T \beta_2 + u_{2i},\\
                        Y_{1i}         &= 1(Y_{1i}^{\star} > 0),\\
                        Y_{2i}         &= Y_{2i}^{\star} \times 1(Y_{1i} = 1).
                  \end{align*}
            \item The variables $(Y_{1i}^{\star}, Y_{2i}^{\star})$ are latent (unobserved).
            \item The observed variables are $( Y_{1i}, Y_{2i}, X_{1i}, X_{2i})$.
            \item Effectively, $Y_{2i}^{\star}$ is observable only when $Y_{1i} = 1$, equivalently when $Y_{1i}^{\star} > 0$.
            \item Typically, the second equation is of interest, e.g. the coefficient $\beta_2$. 
      \end{itemize}
\end{frame}

\begin{frame}{Type-2 Tobit Models: Estimation}
      \begin{itemize}
            \item The Type-2 Tobit model is a classical selection model introduced by Heckman (1976).
            \item It is conventional to assume that the error terms $(u_{1i}, u_{2i})$ are independent of $X_i = (X_{1i}, X_{2i})$.
            \item For details of Heckman's estimation procedure, see the attached pdf file．
            \item Heckman's estimation is one of the parametric approaches, 
                  as he imposes the following parametric distributional assumptions on the joint distribution of the errors:
                  \begin{align*}
                        \begin{bmatrix}
                              u_{1i} \\ u_{2i}
                        \end{bmatrix}
                        \sim 
                        \text{Normal}
                        \left(
                              \begin{bmatrix}
                                    0 \\ 0
                              \end{bmatrix},
                              \begin{bmatrix}
                                    1 & \sigma_{12} \\
                                    \sigma_{21} & \sigma_{2}^2
                              \end{bmatrix}
                        \right)
                  \end{align*}
      \end{itemize}
\end{frame}

\begin{frame}{Semiparametric Type-2 Tobit Models}
      \begin{itemize}
            \item Here we do not impose parametric distributional assumptions on the joint distribution of the errors.
            \item Assume that $(u_{1i}, u_{2i})$ are independent of $X_i = (X_{1i}, X_{2i})$.
            \item Then, we obtain 
                  \begin{align*}
                        \E(Y_{2i} \mid X_i, Y_{1i}=1) 
                              &= X_{2i}^T \beta_2 + \E(u_{2i} \mid X_i, Y_{1i}=1)\\
                              &\equiv X_{2i}^T \beta_2 + g(X_{1i}^T\beta_1),
                  \end{align*}
                  where
                  \begin{align*}
                        g(z) = \E(u_{2i} \mid u_{1i}>-z) = 1 - F_{u_2 \mid u_1}(-z),
                  \end{align*}
                  and 
                  $F_{u_2 \mid u_1}(\cdot)$ is the conditional CDF of $u_{2i}$ given $u_{1i}$.    
            \item The functional form of $g(\cdot)$ is unknown.
      \end{itemize}
\end{frame}

\begin{frame}
      \begin{itemize}
            \item The simple regression of $Y_{2i}$ on $X_{2i}$ using the available data yields
                  \begin{align*}
                        & Y_{2i} = X_{2i}^T \beta_2 + g(X_{1i}^T \beta_1) + \epsilon_{2i},\\
                        & \E(\epsilon_{2i} \mid X_i, Y_{1i}=1) = 0,
                  \end{align*}
                  which is a partially linear single index model.
            \item Here we review the following estimation methods:
                  \begin{itemize}
                        \item Powell (1987),
                        \item Ichimura and Lee (1991),
                        \item Gallant and Nychka (1987),
                        \item Heckman (1990)\footnote{He proposes an estimation of intercept.}.
                  \end{itemize} 
      \end{itemize}
\end{frame}

\begin{frame}{Powell (1987)}
      \begin{itemize}
            \item Powell (1987) proposes a two-step estimation procedure.
            \item \alert{Infeasible Estimation}: Define $Z_i = X_{1i}^T \beta_1$. If $Z_i$ were observed, the regression is 
                  \begin{align*}
                        Y_{2i} = X_{2i}^T \beta_2 + g(Z_i) + \epsilon_{2i},
                  \end{align*}
                  which is a partially linear model. 
            \item This can be estimated using Robinson's approach.
            \item Note that the intercept is absorbed by $g(\cdot)$, 
                  and that it must be excluded from $X_{2i}$
                  \footnote{Recall the identification conditions in semiparametric partially linear models.}
                  .
      \end{itemize}
\end{frame}

\begin{frame}{Powell (1987): Feasible Two-Step Estimation}
      \begin{itemize}
            \item In practice, $Z_i$ is not observed. 
            \item We implement the following two-step approach.
            \item \alert{1st Step}: Estimate $\beta_1$ 
                  by a semiparametric binary choice estimator, 
                  or by Powell's (1984) CLAD estimator
                  \footnote{CLAD推定量を使うのは，後ほど触れるType-3 Tobitの2段階推定の方が適切なのでは？}
                  .
            \item \alert{2nd Step}: Let $\hat{\beta}_1$ denote the estimator of $\beta_1$.
            \item Replace $Z_i$ with $\hat{Z}_i = X_{1i}^T \hat{\beta}_1$: $ Y_{2i} = X_{2i}^T \beta_2 + g(\hat{Z}_i) + \epsilon_{2i}$.
            \item Estiamte $\beta_2$ and $g(\cdot)$ by Robinson's estimator.
            \item Note that $\hat{Z}_i = X_{1i}^T \hat{\beta}_1$ is a generated regressor, 
                  and that the asymptotic distribution will differ from the result presented in Chapter 7.
      \end{itemize}
\end{frame}

\begin{frame}{Ichimura and Lee (1991)}
      \begin{itemize}
            \item Ichimura and Lee (1991) propose a joint estimator for $\theta = (\beta_1^T, \beta_2^T)^T$ 
                  based on the nonlinear regression:
                  \begin{align*}
                        Y_{2i} = X_{2i} + g(X_{1i}\beta_1) + \epsilon_{2i}
                  \end{align*}
                  for observations $i$ such that $Y_{2i}$ is observed.
            \item Their objective function is given by 
                  \begin{align*}
                        Q_n(\theta) 
                              = \frac{1}{n} \sum_{i=1}^n
                                1(X_i \in \mathcal{X}) 
                                \left[
                                    Y_i 
                                    - X_{2i}^T \beta_2 
                                    - \hat{g}(X_{1i}^T \beta_1)
                                \right]^2,
                  \end{align*}
                  where
                  \begin{align*}
                        \hat{g}(X_{1i}^T \beta_1) 
                              = \frac
                                    {
                                          \sum_{j \neq i} (Y_{2j} - X_{2j}^T\beta_2)
                                          K_h\left(
                                                \frac{(X_{1i} - X_{1j})^T \beta_1}{h}
                                          \right)
                                    }{
                                          \sum_{j \neq i} 
                                          K_h\left(
                                                \frac{(X_{1i} - X_{1j})^T \beta_1}{h}
                                          \right)
                                    }
                  \end{align*}
                  is a leave-one-out NW estimator of 
                  $\E(Y_{2i} - X_{2i}^T \beta_2 \mid X_{1i}^T \beta_1)$
                  \footnote{教科書はleave-one-outになっていない．}
                  .
      \end{itemize}
\end{frame}

\begin{frame}
      \begin{itemize}
            \item Their estimator is an extension of a NLLS Heckit estimator, which is based on the equation
                  \begin{align*}
                        Y_{2i} = X_{2i}^T \beta_2 + \sigma_{12} \lambda(X_{1i}^T\beta_1) + \epsilon_{2i}.
                  \end{align*}
            \item Such estimators \alert{ignore the first equation in the system}.
            \item This is convenient as it simplifies the estimation.
            \item However, ignoring relevant information \alert{reduces efficiency}.
            \item Ichimura and Lee derive the asymptotic normality:
                  \begin{align*}
                        \sqrt{n}(\hat{\theta} - \theta) \darrow \text{Normal} (0, A^{-1} \Sigma A^{-1}).
                  \end{align*}
            \item They also derive consistent estimators $\hat{A}$ and $\hat{\Sigma}$.
      \end{itemize}
\end{frame}

\begin{frame}{Gallant and Nychka (1987): Semi-Nonparametric MLE}
      \begin{itemize}
            \item Gallant and Nychka suggest approximating the joint density of the error terms 
                  $f(u_{1i}, u_{2i})$ by a series expansion:
                  \begin{align*}
                        \tilde{f}(u_{1i}, u_{2i}) 
                              = \exp \left[
                                          -\frac{u_{1i}^2}{2 \sigma_{1}^2}  -\frac{u_{2i}^2}{2 \sigma_{2}^2} 
                                     \right]
                                \left[
                                    \sum_{j=0}^K \sum_{k=0}^K \gamma_{jk} u_{1i}^j u_{2i}^k
                                \right].
                  \end{align*}
            \item $\tilde{f}(u_{1i}, u_{2i})$ is a baseline distribution of a joint normal expression.
            \item This is accompanied by a power series expansion allowing for a general form of the CDF.
            \item Using the above joint density formula, we can compute $f(u_{1i}, u_{2i})$ and then
                  construct a log-likelihood function.
            \item Maximizing the log-likelihood function, we obtain estimators of $\beta_1$ and other parameters.
            \item The estimator has consistency under $K \to \infty$, and $\frac{K}{n} \to 0$ as $n \to \infty$.
      \end{itemize}
\end{frame}

\begin{frame}
      \begin{itemize}
            \item Coppejans and Gallant (2002) show that one can use data-driven methods 
                  to select the power series expansion terms when estimating $f(u_{1i}, u_{2i})$.
            \item Newey (1999) proposes a two-step series-based estimation method. 
                  \begin{itemize}
                        \item First, estimate $\beta_1$ efficiently.
                        \item Second, select $\beta_2$ solving an efficient score equation.
                  \end{itemize}
            \item For details of nonparametric series methods, see Li and Racine (2007, Chapter 15).
            \item 教科書にタイポが多いのと，元論文にアクセスできないのとで，不正確な内容が含まれているかもしれないため，
                  何かおかしい点があれば指摘していただきたいです．
      \end{itemize}
\end{frame}

\begin{frame}{Heckman (1990): Intercept Estimation}
      \begin{itemize}
            \item In the semiparametric Type-2 Tobit model, 
                  we cannot identify an intercept term, which cannot be separated from $g(\cdot)$.
            \item One might be interested in the intercept, for example, 
                  when determining ``wage gaps'' between unionized and non-unionized workers, or
                  when decomposing wage differentials between different socioeconomic groups, etc $\cdots$.
      \end{itemize}
\end{frame}

\begin{frame}
      \begin{itemize}
            \item Letting $\mu$ denote the intercept,
                  we write 
                  \begin{align*}
                        Y_{2i}^{\star} = \mu + \tilde{X}_{2i}^T \delta + u_{2i},
                  \end{align*}
                  where $X_{2i} = (1, \tilde{X}_{2i}^T)^T$, and $\beta_2 = (\mu, \delta^T)^T$.
            \item Then, we obtain 
                  \begin{align*}
                        \E(Y_{2i} \mid X_i, Y_{1i}=1) = \mu + \tilde{X}_{2i}^T \delta + g(X_{1i}^T\beta_1).
                  \end{align*}
            \item Recall the definition of $g(\cdot)$:
                  \begin{align*}
                        g(z) = \E(u_{2i} \mid u_{1i} > -z),
                  \end{align*}
                  which leads to 
                  \begin{align*}
                        &\lim_{z \to \infty} g(z) = \E(u_{2i}) = 0, \text{ or}\\
                        &\lim_{z \to \infty} \E(Y_{2i} - \tilde{X}_{2i}^T \delta \mid Y_{1i} = 1, X_{1i}^T \beta_1 > z) = \mu.
                  \end{align*}
      \end{itemize}
\end{frame}

\begin{frame}
      \begin{itemize}
            \item Heckman (1990) suggests to use the observations such that 
                  $\E(u_{2i} \mid Y_{1i} =1) = g(X_{1i}^T \beta_1)$, i.e., 
                  the observations such that $g(\cdot)$ satisfies $g(-\infty) = 0$.
            \item Thus, $\mu$ can be rewritten as
                  \begin{align*}
                        \mu = \E(Y_{2i} - \tilde{X}_{2i}^T \delta \mid Y_{1i}=1, X_{1i}^T\beta_1 > \gamma_n),
                  \end{align*}
                  where $\gamma \to \infty$ is a bandwidth.
            \item This can be estimated by 
                  \begin{align*}
                        \tilde{\mu} = \frac{
                              \sum_{i=1}^n (Y_{2i} - \tilde{X}_{2i}\alert{\hat{\delta}}) Y_{1i} 1(X_{1i}^T \alert{\hat{\beta}_1} > \gamma_n)
                        }{
                              \sum_{i=1}^n  Y_{1i} 1(X_{1i}^T \alert{\hat{\beta}_1} > \gamma_n)
                        }.
                  \end{align*}
      \end{itemize}
\end{frame}

\begin{frame}{Extension: Andrews and Schafgans (1998)}
      \begin{itemize}
            \item Since the indicator function $1(\cdot)$ is not differentiable,
                  it is difficult to examine the asymptotic distribution of $\tilde{\mu}$.
            \item Andrews and Schafgans (1998) suggest to replace the indicator function 
                  with a smoothed non-decreasing CDF $s(\cdot)$, which satisfies
                  \begin{align*}
                        & s(z) = 0 \text{ for } z \leq 0, \\
                        & s(z) = 1 \text{ for } z \geq b \text{ for some } 0 < b < \infty, \text{ and} \\
                        & s(\cdot)  \text{ has third bounded derivatives.} 
                  \end{align*} 
            \item They estimate $\mu$ by
                  \begin{align*}
                        \hat{\mu} = \frac{
                              \sum_{i=1}^n (Y_{2i} - \tilde{X}_{2i}\alert{\hat{\delta}}) Y_{1i} s(X_{1i}^T \alert{\hat{\beta}_1} > \gamma_n)
                        }{
                              \sum_{i=1}^n  Y_{1i} s(X_{1i}^T \alert{\hat{\beta}_1} > \gamma_n)
                        }.
                  \end{align*}
            \item They find that the asymptotic distribution has a non-standard rate, depending on the distribution of $X_{1i}^T \beta_1$.
      \end{itemize}
\end{frame}


\section{Semiparametric Type-3 Tobit Models}

\begin{frame}{Type-3 Tobit Models}
      \begin{itemize}
            \item The Type-3 Tobit model is the four equation system:
                  \begin{align*}
                        Y_{1i}^{\star} &= X_{1i}^T \beta_1 + u_{1i},\\
                        Y_{2i}^{\star} &= X_{2i}^T \beta_2 + u_{2i},\\
                        Y_{1i}         &= \max\{ Y_{1i}^{\star}, 0 \},\\
                        Y_{2i}         &= Y_{2i}^{\star} \times 1(Y_{1i} > 0).
                  \end{align*}
            \item The difference from the Type-2 is that \alert{$Y_{1i}$ is censored} than binary. 
            \item We observe $Y_{2i}^{\star}$ only when there is no censoring on $Y_{1i}^{\star}$.
            \item Typically, the second equation is of interest, e.g. the coefficient $\beta_2$. 
      \end{itemize}
\end{frame}

\begin{frame}{Type-3 Tobit Models: Parametric Approaches}
      \begin{itemize}
            \item Parametric approaches to estimate the Type-3 Tobit models impose 
                  parametric distributional assumptions on the joint distribution of the errors.
            \item Vella (1992, 1998)
            \item Wooldridge (1994)
            \item See the attached file for details.
      \end{itemize}
\end{frame}

\begin{frame}{Semiparametric Type-3 Tobit Models}
      \begin{itemize}
            \item Here we do not assume that the joint distribution of $(u_{1i}, u_{2i})$ is known.
            \item Instead, we have
                  $\E(u_{2i} \mid u_{1i}) = g(u_{1i})$,
                  where $g(\cdot)$ is an unknown function.
            \item In this case, it is easy to see that 
                  $\E(Y_{2i} \mid X_i, u_{1i}) = X_{2i}^T \beta_2 + g(u_{1i})$.
            \item Thus, we obtain 
                  \begin{align*}
                        &Y_{2i} = X_{2i}^T \beta_2 + g(u_{1i}) + v_{2i},\\
                        &\E(v_{2i} \mid u_{1i}, Y_{1i} > 0) = 0.
                  \end{align*}
      \end{itemize}
\end{frame}

\begin{frame}{Semiparametric Type-3 Tobit Models: Estimation}
      \begin{itemize}
            \item If $u_{1i}$ were known, this would be a partially linear model.
            \item In practice, $u_{1i}$ is unknown.
            \item We can estimate $u_{1i}$ by Tobit, CLAD, etc $\cdots$.
            \item Here we review the following estimation methods:
            \begin{itemize}
                  \item Li and Wooldridge (2002),
                  \item Chen (1997),
                  \item Honore, Kyriazidou and Udry (1997),
                  \item Lee (1994),
                  \item the semiparametric Type-2 Tobit estimator besed on Ichimura (1993); and Ichimura and Lee (1991).
            \end{itemize}
      \end{itemize}
\end{frame}

\begin{frame}{Li and Wooldridge (2002)}
      \begin{itemize}
            \item Li and Wooldridge suggest a multistep method to estimate $\beta_2$.
            \item \alert{1st Step}: Estimate $\beta_1$, for example, by Powell's (1984) CLAD estimator
                  \footnote{Powell's CLAD (censored least absolute deviation) estimator will be reviewed below in Chapter 11.}
                  .
                  We assume that for the 1st step 
                  there is a $\sqrt{n}$-consistent, and asymptotically normally distributed, estimator for $\beta_1$.
            \item \alert{2nd Step}: Replacing $u_{1i}$ with $\hat{u}_{1i}$,
                  we implement Robinson's approach to estimate $\beta_2$.
            \item They establish the $\sqrt{n}$-normality of their estimator $\hat{\beta}_2$:
                  \begin{align*}
                        \sqrt{n}(\hat{\beta}_2 - \beta_2) \darrow \text{Normal} (0, \Sigma).
                  \end{align*}
            \item $\text{Avar} (\hat{\beta}_2)$ can be consistently estimated.
      \end{itemize}
\end{frame}

\begin{frame}
      \begin{itemize}
            \item Li and Wooldridge's (2002) estimator does not achieve the semiparametric efficiency bound.
                  Efficient estimation can usually be achieved by a one-step procedure, 
                  where $\beta_1$ and $\beta_2$ are estimated simultaneously as in Ai (1997).
            \item Powell's CLAD estimator is a parametric approach, i.e., 
                  the generated regressor is estimated from a parametric model.
                  Ahn and Powell (1993) suggest to estimate $\beta_1$ in the 1st stage using a nonparametric regression model.
      \end{itemize}
\end{frame}


\begin{frame}{Chen (1997)}
      \begin{itemize}
            \item Assume that $(u_{1i}, u_{2i})$ is independent of $(X_{1i}, X_{2i})$.
            \item Under the above assumption, it holds that
                  \begin{align*}
                        &\E(Y_{2i} \mid X_{1i}, X_{2i}, u_{1i}>0, X_{1i}^T\beta_1>0, Y_{1i}>0) \\
                        &\qquad= \E(Y_{2i} \mid u_{1i}>0, X_i) \\
                        &\qquad= X_{2i}^T \beta_2 + \alpha_0,
                  \end{align*}
                  where $\alpha_0$ is a constant. 
                  Note that $\alpha_0$ is not the intercept of the original model.
            
      \end{itemize}
\end{frame}

\begin{frame}
      \begin{itemize}
            \item \alert{1st Step}: Estimate $\beta_1$ consistently 
                  by Honore and Powell (1984), or by Powell's CLAD.
                  Let $\hat{\beta}_1$ denote the consistent estimator of $\beta_1$.
            \item \alert{2nd Step}: Run the following least squares:
                  \begin{align*}
                        \min_{\beta_2, \alpha}
                                \frac{1}{n} \sum_{i=1}^n 
                                1\{ %trimming
                                    Y_{1i} - X_{1i}^T \alert{\hat{\beta}_1} > 0,
                                    X_{1i}^T \alert{\hat{\beta}_1} > 0                                
                                  \} 
                                (Y_{2i} - X_{2i}^T \beta_2 - \alpha)^2.
                  \end{align*}
            \item A problem arising with Chen's (1997) estimator is that 
                  it may trim out too many observations, which leads to inefficient estimation.
            \item Chen (1997) suggests an alternative estimator that trims far fewer data points in finite-sample applications.
      \end{itemize}
\end{frame}


\begin{frame}{Honore, Kyriazidou and Udry (1997)}
      \begin{itemize}
            \item Honore, Kyriazidou and Udry (1997) suggest to relax 
                  the normality assumption as it can be seen in Heckman (1979).
            \item Instead, they assume that 
                  the distribution of the error terms $(u_{1i}, u_{2i})$ 
                  given regressors $X_i$ is symmetric, 
                  with arbitary heteroscedasticity permitted.
            \item In this case, conditional on the sample selection, 
                  the conditional distribution is no longer symmetric.
            \item Their basic idea is that 
                  \alert{
                  $u_{2i}$ is symmetrically distributed around $0$ 
                  }
                  if one estimate $\beta_2$  using 
                  \alert{
                  observations for which $-X_{1i}^T \beta_1 < u_{1i} < X_{1i}^T \beta_1$} 
                  (i.e., $0 < Y_{1i} < 2 X_{1i}^T \beta_1$).
      \end{itemize}
\end{frame}

\begin{frame}
      \begin{itemize}
            \item Honore, Kyriazidou and Udry (1997) suugest the following estimation method.
            \item \alert{1st Step}: Estimate $\beta_1$ consistently, 
                  for example, by Powell's CLAD.
                  Let $\hat{\beta}_1$ denote the consistent estimator of $\beta_1$.
            \item \alert{2nd Step}: Run the following least absolute deviations:
                  \begin{align*}
                        \min_{\beta_2} \frac{1}{n} \sum_{i=1}^n
                                       1\{
                                          0 < Y_{1i} < 2 X_{1i}^T \hat{\beta}_1
                                       \}
                                       \mid Y_{2i} - X_{2i}^T \beta_2 \mid.
                  \end{align*}
            \item They establish the $\sqrt{n}$-normality of their estimator.
      \end{itemize}
\end{frame}


\begin{frame}{Lee (1994)}
      \begin{figure}
            \includegraphics[width = 1.0 \textwidth]{material/Lee1994_selection.png}
      \end{figure}
\end{frame}

\begin{frame}{Comparing the 4 Estimators}
      \begin{center}
      \small{
            \begin{tabular}{c|cccc}
                  \hline
                        & LW & Chen & HKU & Lee \\
                  \hline
                  Kernel Methods & Required & - & - & Required \\
                  Smoothing Parameter Choice & Insensitive & - & - & Insensitive \\
                  \hline
            \end{tabular}
      }
      \end{center}
      \begin{itemize}
            \item In general, nonparametric kernel methods are sensitive to the choice of smoothing parameters.
            \item Lee (1994); Min, Sheu and Wang (2003) suggest by Monte-Carlo simulations that 
                  the estimators of LW(2002) and Lee (1994) are fairly insensitive to the choice of smoothing parameters.
            \item The reason is that 
                  the semiparametric estimators depend on the average of nonparametric estimators, 
                  which are less sensitive to different values of smoothing parameters than a pointwise nonparametric kernel estimator.
      \end{itemize}
\end{frame}

\begin{frame}{Christofides, Li, Liu and Min (2003)}
      \begin{center}
      \small{
            \begin{tabular}{c|cccc}
                  \hline
                        & LW & Chen & HKU & Lee \\
                  \hline
                  Dependence Assumption & Yes & Yes & No & Yes \\
                  Symmetry Assumption & No & No & Yes & No \\ 
                  \hline
            \end{tabular}
      }
      \end{center}
      \begin{itemize}
            \item ``Dependence Assumption'' means that 
                  one need to assume that 
                  $(u_{1i}, u_{2i})$ is independent of $(X_{1i}, X_{2i})$.
            \item ``Symmetry Assumption'' means that one need to assume that 
                  the distribution of the error terms $(u_{1i}, u_{2i})$ 
                  given regressors $X_i$ is symmetric.
            \item The symmetry condition is neither weaker nor stronger than the independence condition.
      \end{itemize}
\end{frame}



\section{Tests for the Exisitence of Sample Selection and Model Specification}

\begin{frame}{Test for the Exisitence of Sample Selection}
      \begin{itemize}
            \item Let us test whether sample selection exists or not.
            \item Consider the following null hypothesis:
                  \begin{align*}
                        &\H_{0}^a: \, \E(u_2|u_1) = 0, \\
                        &\H_{1}^a: \, \E(u_2|u_1) \neq 0.
                  \end{align*}
            \item A test statistic for $\H_{0}^a$ is proposed 
                  by Li and Wang (1998); and Zheng (1996):
                  \begin{align*}
                        \tilde{I}_n^a 
                              = \frac{1}{n_1 (n_1 - 1)}
                                \sum_{i=1}^{n_1} \sum_{j \neq i}^{n_1}
                                \hat{u}_{2i} \hat{u}_{2j} K_h (\hat{u}_{1i} \hat{u}_{1j}),
                  \end{align*}
                  where $n_1$ denotes the number of observations with $Y_{2i} > 0$, and 
                  $\hat{u}_{1i} \text{ and } \hat{u}_{2i}$ are the OLS residuals.
      \end{itemize}
\end{frame}

\begin{frame}
      \begin{itemize}
            \item Under conditions 10.1, 10.2, and 10.3, we obtain 
                  (as $n_1 \to \infty, \, h \to 0,\, n_1 h \to \infty$),
                  \begin{align*}
                        n_1 h^{\frac{1}{2}}\frac{\tilde{I}_n^a}{\hat{\sigma}_a} \darrow \text{Normal} (0,1) \text{ under } \H_{0}^a
                  \end{align*}
                  and 
                  \begin{align*}
                        \P \left(
                              \left|
                                    n_1 h^{\frac{1}{2}}\frac{\tilde{I}_n^a}{\hat{\sigma}_a}  
                              \right| > C
                        \right) \to 1 \text{ for any } C>0 \text{ under } \H_{1}^a,
                  \end{align*}
                  where $\hat{\sigma}_a^2 
                              = \dfrac{2h}{n_1(n_1 - 1)}
                                \sum_{i=1}^{n_1} \sum_{j \neq i}^{n_1}
                                \hat{u}_{2i}^2 \hat{u}_{2j}^2 K_h^2 (\hat{u}_{1i} - \hat{u}_{1j})$.
      \end{itemize}
\end{frame}

\begin{frame}{Test for Model Specification}
      \begin{itemize}
            \item Suppose that we reject $\H_0^a$, that is, we consider there exists sample selection.
            \item Consider the following null hypothesis:
                  \begin{align*}
                        &\H_{0}^b: \, \E(Y_2|X_2, u_1) = X_2^T \beta_2 + u_1 \gamma, \\
                        &\H_{1}^a: \, \E(Y_2|X_2, u_1) = X_2^T \beta_2 + g(u_1) \text{ with } g(u_1) \neq u_1 \gamma.
                  \end{align*}
            \item A test statistic for $\H_{0}^a$ is proposed 
                  by Li and Wang (1998):
                  \begin{align*}
                        \tilde{I}_n^b 
                              = \frac{1}{n_1 (n_1 - 1)}
                                \sum_{i=1}^{n_1} \sum_{j \neq i}^{n_1}
                                \tilde{u}_{2i} \tilde{u}_{2j} K_h (\hat{u}_{1i} \hat{u}_{1j}),
                  \end{align*}
                  where $n_1$ denotes the number of observations with $Y_{2i} > 0$, and 
                  $\hat{u}_{1i} \text{ and } \tilde{u}_{2i}$ are the residuals 
                  from OLS $\beta_1$ and Li and Wooldridge's (2002) $\beta_2$, respectively.
      \end{itemize}
\end{frame}

\begin{frame}
      \begin{itemize}
            \item Under conditions 10.1, 10.2, and 10.3, we obtain 
                  (as $n_1 \to \infty, \, h \to 0,\, n_1 h \to \infty$),
                  \begin{align*}
                        n_1 h^{\frac{1}{2}}\frac{\tilde{I}_n^b}{\hat{\sigma}_b} \darrow \text{Normal} (0,1) \text{ under } \H_{0}^b
                  \end{align*}
                  and 
                  \begin{align*}
                        \P \left(
                              \left|
                                    n_1 h^{\frac{1}{2}}\frac{\tilde{I}_n^b}{\hat{\sigma}_b}  
                              \right| > C
                        \right) \to 1 \text{ for any } C>0 \text{ under } \H_{1}^b,
                  \end{align*}
                  where $\hat{\sigma}_b^2 
                              = \dfrac{2h}{n_1(n_1 - 1)}
                                \sum_{i=1}^{n_1} \sum_{j \neq i}^{n_1}
                                \tilde{u}_{2i}^2 \tilde{u}_{2j}^2 K_h^2 (\hat{u}_{1i} - \hat{u}_{1j})$.
      \end{itemize}
\end{frame}

\section{Nonparametric Sample Selection Model}

\begin{frame}{Das, Newey, and Vella (2003)}
      \begin{itemize}
            \item See Das, Newey, and Vella (2003); and Li and Racine (2007, Section 10.4).
      \end{itemize}
\end{frame}



\section{References}

\begin{frame}{References}
      \begin{itemize}
            \item Textbooks and Lecture Notes:
                  \begin{itemize}
                        \item Li, Q., and J. S. Racine (2007). 
                              \textit{Nonparametric Econometrics: Theory and Practice,} 
                              Princeton University Press.
                        \item 末石直也 (2015) 『計量経済学：ミクロデータ分析へのいざない』日本評論社．
                        \item 末石直也 (2024) 『データ駆動型回帰分析：計量経済学と機械学習の融合』日本評論社．
                        \item 西山慶彦，新谷元嗣，川口大司，奥井亮 (2019) 『計量経済学』有斐閣．
                        \item ECON 718 NonParametric Econometrics (Bruce Hansen, Spring 2009, University of Wisconsin-Madison).
                        \item セミノンパラメトリック計量分析（末石直也，2014年度後期，京都大学）．
                  \end{itemize}
      \end{itemize}
\end{frame}






\end{document}