\documentclass[xcolor=svgnames,dvipdfmx,cjk]{beamer} 
\AtBeginDvi{\special{pdf:tounicode 90ms-RKSJ-UCS2}} 
\usetheme{Madrid}
\setbeamercolor{background canvas}{bg=Snow}
\usecolortheme[named=RoyalBlue]{structure}
\usefonttheme{professionalfonts}
\setbeamertemplate{theorems}[numbered]
\newtheorem{thm}{Theorem}[section]
\newtheorem{proposition}[thm]{Proposition}
\theoremstyle{example}
\newtheorem{exam}[thm]{Example}
\newtheorem{remark}[thm]{Remark}
\newtheorem{question}[thm]{Question}
\newtheorem{prob}[thm]{Problem}
\usepackage{bbm}
\usepackage{ascmac}

\begin{document} 

%%%%%講演に関する情報%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title[Li and Racine (2007) Section 5.4]{Nonparametric Estimation of Conditional PDF} 
\author[Y. Matsumura]{Yasuyuki Matsumura}          
\institute[]{Graduate School of Economics, Kyoto University} 
\date{\today}


%%%%%タイトルページ%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}                  
\titlepage                     
\end{frame}


%%%%%目次のページ%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}                  
\tableofcontents
\end{frame}
%本文中に挿入するSECTION環境によって定義された目次が自動で反映される


%%%%%基本のコマンド%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{箇条書き} :目次の設定
% \begin{frame} & \end{frame} :1ページのスライドを作成
% \begin{itemaize} :箇条書き
%   \item A
%   \item B
%   \item C
% \end{itemize}
% \pause :スライドを一旦停止

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Multivariate Dependent Variables Case}

\begin{frame}{Introduction}
  \begin{itemize}
    \item We consider the estimation of the conditional PDF $g(Y|X)$
          when both $Y$ and $X$ are general multivariate vectors.
  \end{itemize}
\end{frame}

\begin{frame}{Settings}
\begin{itemize}
  \item $ Z = (X, Y) = (Z^c, Z^d) $
  \item $ Z^d $ consists of $r$ discrete variables.
   \begin{itemize}
    \item $ X^d \in \mathbb{R}^{r_x} ,\, Y^d \in \mathbb{R}^{r_y} $.
   \end{itemize}
  \item $ Z^c $ consists of $q$ continuous variables.
   \begin{itemize}
    \item $ X^c \in \mathbb{R}^{q_x} ,\, Y^c \in \mathbb{R}^{q_y} $.
   \end{itemize}
  \item $r = r_x + r_y,\,q = q_x + q_y$.
\end{itemize}  
\end{frame}

\begin{frame}{Kernel Methods}
  \begin{itemize}
    \item Let $Z_{is}^d$ denote the $s$-th component of $Z_i^d$.
    \item Define a univariate kernel function
          \begin{align*}
            l(Z_{is}^d , Z_{js}^d , \lambda_s) = 
            \left\{
            \begin{array}{l}
              1 - \lambda_s 
              \quad
              \text{if} \, 
              Z_{is}^d = Z_{js}^d , \\
              \dfrac{\lambda_s}{c_s-1}
              \quad
              \text{if} \, 
              Z_{is}^d \neq Z_{js}^d .
            \end{array}
            \right.
          \end{align*}
    \item The product kernel is given by
    \begin{align*}
      L_{\lambda, Z_{i}^d , Z_{j}^d} 
      =
      \prod_{s=1}^{r}  l(Z_{is}^d , Z_{js}^d , \lambda_s).
    \end{align*}
  \end{itemize}
\end{frame}

\begin{frame}{Kernel Methods}
  \begin{itemize}
    \item Let $Z_{is}^c$ denote the $s$-th component of $Z_i^c$.
    \item Define a univariate kernel function $w(\cdot)$ in the same way as Section 5.1.
    \item Then, the product kernel is given by 
          \begin{align*}
            W_{h, Z_i^c , Z_j^c}
            =
            \prod_{s=1}^{q} \frac{1}{h_s} w \left( \frac{Z_{is}^c - Z_{js}^c}{h_s} \right).
          \end{align*}
    \item Define 
          $K_{\gamma, Z_i, z} 
           = L_{\lambda, Z_{i}^d , z} 
             \times 
             W_{h, Z_i^c , z^c}$.
    \item Apply the same notation $L(\cdot)$ and $W(\cdot)$ to Y and X.
  \end{itemize}
\end{frame}

\begin{frame}{Kernel Methods}
  \begin{itemize}
    \item We estimate 
          \begin{align*}
            g(y|x) = \frac{f(x,y)}{\mu(x)}
          \end{align*}
          by the following nonparametric estimator:
          \begin{align*}
            \hat{g}(y|x) = \frac{\hat{f}(x,y)}{\hat{\mu}(x)} ,
          \end{align*}
          where
          \begin{align*}
            \hat{f}(x,y) 
              &= \hat{f}(z) 
               = \frac{1}{n} \sum_{i=1}^n K_{\gamma, Z_i, z} , \\
            \hat{\mu}(x) 
              &= \frac{1}{n} \sum_{i=1}^n K_{\gamma, X_i, x}.
          \end{align*}
  \end{itemize}
\end{frame}

\begin{frame}{Bandwidth Selection: Least Squares CV}
  \begin{itemize}
    \item Consider the case where the independent variables are all relevent.
          \begin{itemize}
            \item Irrelevant independent variables will be asymptotically smoothed out.
          \end{itemize}
    \item As in Section 5.2, 
          we choose the tuning parameters $h$ and $\lambda$ 
          that minimize a sample analogue of $I_{1n} - 2 I_{2n}$:
          \begin{align*}
            CV(h, \lambda) 
            = \frac{1}{n} \sum_{i=1}^{n} \frac{\hat{G}_{-i}(X_i)}{\hat{\mu}_{-i}(X_i)^2}
            - \frac{2}{n} \sum_{i=1}^{n} \frac{\hat{f}_{-i}(X_i)}{\hat{\mu}_{-i}(X_i)}.
          \end{align*}
  \end{itemize}
\end{frame}

\begin{frame}{Bandwidth Selection: Least Squares CV}
  \begin{itemize}
    \item Take $\hat{h}$ and $\hat{\lambda}$ as the CV choices.
    \item Let ${h_s}^0$ and ${\lambda_s}^0$ denote the minimizers of the leading term of $CV(h, \lambda)$.
  \end{itemize}
  \begin{itembox}[l]{Racine et al. [2004]}
    Where 
    ${h_s}^0 = c_{1s} n^{\frac{-1}{q+4}}$ 
    and
    ${\lambda_s}^0 = c_{2s} n^{\frac{-2}{q+4}}$,
    it holds that
    \begin{align*}
      \frac{\hat{h}_s}{{h_s}^0} \xrightarrow{p} 1, 
      \quad 
      \frac{\hat{\lambda}_s}{{\lambda_s}^0} \xrightarrow{p} 1.
    \end{align*}
  \end{itembox}
\end{frame}

\begin{frame}{Bandwidth Selection: Least Squares CV}
  \begin{itembox}[l]{Theorem 5.4}
    Under several assumptions given in Racine et al. (2004), 
    \begin{align*}
      \frac{\hat{h}_s-{h_s}^0}{{h_s}^0} 
       &= O_p \left( n^{\frac{-\alpha}{q+4}} \right),
        \quad \alpha = \min \left\{ 2, \frac{q}{2} \right\},\\
      \frac{\hat{\lambda}_s-{\lambda_s}^0}{{\lambda_s}^0} 
       &= O_p \left( n^{-\beta} \right),
        \quad \beta = \min \left\{ \frac{1}{2}, \frac{4}{q+4} \right\}.
    \end{align*}    
  \end{itembox}
\end{frame}

\begin{frame}{Bandwidth Selection: Least Squares CV}
  \begin{itembox}[l]{Theorem 5.5 (Asymptotic Normality)}
    \begin{align*}
      \sqrt{n h_1 \cdots h_q} 
      &\left(
        \hat{g}(y|x) - g(y|x) 
         - \sum_{s=1}^{q} {\hat{h}_s}^2 B_{1s}(z)
         - \sum_{s=1}^{r} \hat{\lambda}_s B_{2s}(z)
      \right) \\
      &\xrightarrow{d} N (0, \Omega(z)),
    \end{align*}
   where $B_{1s}(z), B_{2s}(z)$, $\Omega(z)$ are defined by
    \begin{align*}
      B_{1s}(z) &=
      \left\{
            \begin{array}{l}
                \frac{1}{2} \kappa_2 \frac{f_{ss}(x,y)}{\mu(x)}\, 
                (s = 1,\cdots, q_y) , \\
                \frac{1}{2} \kappa_2 \frac{f_{ss}(x,y) - \mu_{ss}(x)g(y|x)}{\mu(x)}\,
                (s = q_y+1 , \cdots , q),
            \end{array}
       \right. \\
      B_{2s}(z) &=
       \left\{
            \begin{array}{l}
                \frac{1}{c_s-1} \sum_{v^d \in \mathcal{D}}
                \mathbf{1}_s(v^d,z^d)f(z^c,v^d)\,
                (s = 1, \cdots , r_y), \\
                \frac{1}{c_s-1} \sum_{u^d \in \mathcal{D}}
                \mathbf{1}_s(u^d,x^d) 
                \frac{f(z^c,y^d,u^d) - g(y|x)\mu(x^c,u^d)}{\mu(x)}\,
                (s = r_y+1, \cdots , r), \\
            \end{array}
       \right. \\
       \Omega(z) &= \frac{\kappa^q g(y|x)}{\mu(x)}.
    \end{align*}
  \end{itembox}
\end{frame}

\section{Proof of Theorem 5.5}

\begin{frame}{Stochastic Equicontinuity}
  \begin{itemize}
    \item Take an empirical process ${v}_n(t)$. 
    \item We say $v_n(t)$ is stochastic equicontinuous\footnote{確率的同程度連続．} at $t=t_0$ if
          \begin{align*}
            \text{For} \, & ^{\forall}\epsilon>0, ^{\forall}\eta>0,  ^{\exists}\delta>0; \\
                          & \limsup_{n \to \infty} \mathbb{P} \left(
                            \sup_{t \in T, \, \rho(t, t_0)<\delta} |v_n(t)-v_n(t_0)|>\eta  
                          \right) 
                          < \epsilon .
          \end{align*}
  \end{itemize}
\end{frame}

\begin{frame}{Proof of Theorem 5.5}
  \begin{itemize}
    \item The smoothing parameters $\hat{h}_s, \hat{\lambda}_s$ 
          which we take as the minimizers of $CV(h, \lambda)$ are stochastic.
    \item On the other hand, the parameters $h_s, \lambda_s$ 
          that are defined as 
          $h_s = c_{1s} \times n^{\frac{-1}{q+4}}, \lambda_s = c_{2s} \times n^{\frac{-2}{q+4}}$
          (which satisfy $\frac{\hat{h}_s}{h_s} \xrightarrow{p} 1, \frac{\hat{\lambda}_s}{\lambda_s} \xrightarrow{p} 1$)
          are nonstochastic.
    \item By using stochastic equicontinuity arguments, 
          we know that the asymptotic distribution of $\hat{g}(y|x)$ is the same
          WHETHER we use stochastic $\hat{h}_s, \hat{\lambda}_s$ 
          OR nonstochastic $h_s, \lambda_s$.
    \item Therefore, we consider only the nonstochastic smoothing parameter case. 
  \end{itemize} 
\end{frame}

\begin{frame}{Proof of Theorem 5.5}
  \begin{itemize}
    \item To derive the asymptotic normality of $\hat{g}(y|x)$, we write
          \begin{align*}
            \hat{g}(y|x) - g(y|x) 
            =
            \frac{(\hat{g}(y|x)-g(y|x))\hat{\mu}(x)}{\hat{\mu}(x)}
            \equiv
            \frac{\hat{m}(y,x)}{\hat{\mu}(x)},
          \end{align*}
          where 
          $\hat{m}(y,x) = (\hat{g}(y|x)-g(y|x))\hat{\mu}(x) = \hat{f}(y,x) - g(y|x)\hat{\mu}(x)$. 
  \end{itemize}
\end{frame}

\begin{frame}{Proof of Theorem 5.5}
  \begin{itemize}
    \item We consider the computation of 
          \begin{itemize}
            \item $\mathbb{E}(\hat{m}(y,x))$,
            \item $\text{var}(\hat{m}(y,x))$,
          \end{itemize}
          so that we will obtain the asymptotic normality of $\hat{m}(y,x)$:
          \begin{align*}
            \sqrt{n h_1 \cdots h_q}
            \{ \hat{m}(y,x) - (\text{bias terms}) \}
            \xrightarrow{d}
            N(0, \kappa^q f(y,x)).
          \end{align*}
    \item Note that we have already established
          \begin{align*}
            \hat{\mu}(x) - \mu(x) 
            = Op \left(
              \sum_{s = q_y +1}^{q} h_s^2 
              +
              \frac{1}{\sqrt{ n h_{q_y+1} \cdots h_q}} 
              \right).
          \end{align*}
    \item Then, we derive the asymptotic normality of $\hat{g}(y|x)$:
          \begin{align*}
            \sqrt{n h_1 \cdots h_q}
            (\hat{g}(y|x) - g(y|x) - (\text{bias temrs}))
            \xrightarrow{d}
            N(0, \Omega(z)).
          \end{align*}
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{References}

\begin{frame}{References}
  \begin{itemize}
    \item Li, Q. and J. S. Racine, (2007). 
          \textit{Nonparametric Econometrics: Theory and Practice,} 
          Princeton University Press.
  \end{itemize}
\end{frame}

\end{document}